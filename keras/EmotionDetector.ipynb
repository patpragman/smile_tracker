{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OJj0OY5dtO9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# My Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17013 images belonging to 3 classes.\n",
      "Found 4251 images belonging to 3 classes.\n",
      "Number of training samples in each class in the training set: {'happy': 7192, 'neutral': 4959, 'sad': 4862}\n",
      "Number of test samples in each class in the testing set: {'happy': 1797, 'neutral': 1239, 'sad': 1215}\n",
      "{0: 1.0, 1: 1.4502923976608186, 2: 1.479226655697244}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, RandomFlip, RandomRotation, RandomZoom, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.metrics import binary_accuracy\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "seed = 415\n",
    "batch_size = 8\n",
    "image_path = \"./images\"\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             validation_split=0.2,\n",
    "                              zoom_range = 0.1, # Randomly zoom image\n",
    "                              width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                              height_shift_range=0.1,\n",
    "                             rotation_range=30\n",
    "                             )\n",
    "\n",
    "\n",
    "# I changed the imagery to grayscale to speed up the training process\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    image_path,\n",
    "    target_size=(227, 227),  # resize for alexnet\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    image_path,\n",
    "    target_size=(227, 227),  # resize for alexnet\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "train_class_counts = train_generator.classes\n",
    "test_class_counts = test_generator.classes\n",
    "\n",
    "train_class_count = dict(zip(train_generator.class_indices.keys(), np.zeros(len(train_generator.class_indices), dtype=int)))\n",
    "test_class_count = dict(zip(test_generator.class_indices.keys(), np.zeros(len(test_generator.class_indices), dtype=int)))\n",
    "\n",
    "for label in train_class_counts:\n",
    "    train_class_count[list(train_generator.class_indices.keys())[int(label)]] += 1\n",
    "\n",
    "for label in test_class_counts:\n",
    "    test_class_count[list(test_generator.class_indices.keys())[int(label)]] += 1\n",
    "\n",
    "print('Number of training samples in each class in the training set:', train_class_count)\n",
    "print('Number of test samples in each class in the testing set:', test_class_count)\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(train_generator.classes)\n",
    "max_val = float(max(counter.values()))\n",
    "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n",
    "print(class_weights)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# we'll use this block to build a model that we can tune in keras-tune\n",
    "import keras_tuner\n",
    "\n",
    "def build_model(hp:keras_tuner.HyperParameters):\n",
    "\n",
    "    filter_possibilities = [16, 32, 64, 96, 128, 256, 384, 512]\n",
    "    dense_size_possibilities = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 4096]\n",
    "    drop_out_possibilities = list(np.linspace(0.2, 0.6, 10))\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.01)\n",
    "\n",
    "    # hyper parameters we care about\n",
    "    activation = hp.Choice(\"activation function\", [\"relu\", \"leaky_relu\"])\n",
    "    if activation == \"leaky_relu\":\n",
    "        activation = leaky_relu\n",
    "\n",
    "    first_filter_count = hp.Choice(\"first filter count\", filter_possibilities)\n",
    "    second_filter_count = hp.Choice(\"second filter count\", filter_possibilities)\n",
    "    third_filter_count = hp.Choice(\"third filter count\", filter_possibilities)\n",
    "    fourth_filter_count = hp.Choice(\"fourth filter count\", filter_possibilities)\n",
    "    fifth_filter_count = hp.Choice(\"fifth filter count\", filter_possibilities)\n",
    "\n",
    "    first_dense_layer_size = hp.Choice(\"first dense layer size\", dense_size_possibilities)\n",
    "    second_dense_layer_size = hp.Choice(\"second dense layer size\", dense_size_possibilities)\n",
    "\n",
    "    first_dense_layer_dropout = hp.Choice(\"drop out for first dense layer\", drop_out_possibilities)\n",
    "    second_dense_layer_dropout = hp.Choice(\"dropout for second dense layer\", drop_out_possibilities)\n",
    "\n",
    "    # optimizer\n",
    "    opt = hp.Choice(\"optimizer\", [\"sgd\", \"adam\"])\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "    # entry point\n",
    "    Conv2D(filters=first_filter_count,\n",
    "           kernel_size=(11,11),\n",
    "           strides=(4,4),\n",
    "           activation=activation,\n",
    "           input_shape=(227,227,1)),  # resized for one chanel\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3,3), strides=(2,2)),  # extract first feature set\n",
    "\n",
    "\n",
    "    Conv2D(filters=second_filter_count,\n",
    "           kernel_size=(5,5), strides=(1,1), activation=activation, padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3,3), strides=(2,2)),  # extract second feature set\n",
    "\n",
    "\n",
    "    Conv2D(filters=third_filter_count, kernel_size=(3,3), strides=(1,1), activation=activation, padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=fourth_filter_count, kernel_size=(3,3), strides=(1,1), activation=activation, padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=fifth_filter_count, kernel_size=(3,3), strides=(1,1), activation=activation, padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3,3), strides=(2,2)),  # extract 3rd feature set\n",
    "\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    # note how this is much smaller than AlexNet\n",
    "    Dense(first_dense_layer_size, activation=activation),\n",
    "    Dropout(first_dense_layer_dropout),\n",
    "    Dense(second_dense_layer_size, activation=activation),\n",
    "    Dropout(second_dense_layer_dropout),\n",
    "    Dense(3, activation='softmax'),  # only 3 choices here\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['categorical_accuracy']\n",
    "              )\n",
    "\n",
    "    model.build(input_shape=(None, 227, 227, 1))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yUhx8OZQIMjE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "85d0c29c-e84b-4b80-a6ff-ded790b0d6d2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 11\n",
      "activation function (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'leaky_relu'], 'ordered': False}\n",
      "first filter count (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 32, 64, 96, 128, 256, 384, 512], 'ordered': True}\n",
      "second filter count (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 32, 64, 96, 128, 256, 384, 512], 'ordered': True}\n",
      "third filter count (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 32, 64, 96, 128, 256, 384, 512], 'ordered': True}\n",
      "fourth filter count (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 32, 64, 96, 128, 256, 384, 512], 'ordered': True}\n",
      "fifth filter count (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 32, 64, 96, 128, 256, 384, 512], 'ordered': True}\n",
      "first dense layer size (Choice)\n",
      "{'default': 4, 'conditions': [], 'values': [4, 8, 16, 32, 64, 128, 256, 512, 1024, 4096], 'ordered': True}\n",
      "second dense layer size (Choice)\n",
      "{'default': 4, 'conditions': [], 'values': [4, 8, 16, 32, 64, 128, 256, 512, 1024, 4096], 'ordered': True}\n",
      "drop out for first dense layer (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.24444444444444446, 0.28888888888888886, 0.3333333333333333, 0.37777777777777777, 0.4222222222222222, 0.4666666666666666, 0.5111111111111111, 0.5555555555555556, 0.6], 'ordered': True}\n",
      "dropout for second dense layer (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.24444444444444446, 0.28888888888888886, 0.3333333333333333, 0.37777777777777777, 0.4222222222222222, 0.4666666666666666, 0.5111111111111111, 0.5555555555555556, 0.6], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'sgd', 'conditions': [], 'values': ['sgd', 'adam'], 'ordered': False}\n",
      "None\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "leaky_relu        |leaky_relu        |activation function\n",
      "96                |96                |first filter count\n",
      "64                |64                |second filter count\n",
      "512               |512               |third filter count\n",
      "32                |32                |fourth filter count\n",
      "128               |128               |fifth filter count\n",
      "256               |256               |first dense layer size\n",
      "4096              |4096              |second dense layer size\n",
      "0.28889           |0.28889           |drop out for first dense layer\n",
      "0.28889           |0.28889           |dropout for second dense layer\n",
      "adam              |adam              |optimizer\n",
      "\n",
      "Epoch 1/100\n",
      "2127/2127 [==============================] - 132s 62ms/step - loss: 1.4952 - categorical_accuracy: 0.3518 - val_loss: 1.0822 - val_categorical_accuracy: 0.4187 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2127/2127 [==============================] - 130s 61ms/step - loss: 1.4357 - categorical_accuracy: 0.3576 - val_loss: 1.1359 - val_categorical_accuracy: 0.3954 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2127/2127 [==============================] - 132s 62ms/step - loss: 1.4188 - categorical_accuracy: 0.3705 - val_loss: 1.0968 - val_categorical_accuracy: 0.4093 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2127/2127 [==============================] - 133s 63ms/step - loss: 1.4270 - categorical_accuracy: 0.3915 - val_loss: 1.0813 - val_categorical_accuracy: 0.4350 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2127/2127 [==============================] - 131s 62ms/step - loss: 1.4107 - categorical_accuracy: 0.3902 - val_loss: 1.0363 - val_categorical_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2127/2127 [==============================] - 132s 62ms/step - loss: 1.3852 - categorical_accuracy: 0.4220 - val_loss: 1.0686 - val_categorical_accuracy: 0.4526 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "2127/2127 [==============================] - 132s 62ms/step - loss: 1.3415 - categorical_accuracy: 0.4628 - val_loss: 0.9850 - val_categorical_accuracy: 0.5018 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "2127/2127 [==============================] - 133s 62ms/step - loss: 1.2520 - categorical_accuracy: 0.5128 - val_loss: 0.9255 - val_categorical_accuracy: 0.5737 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "2127/2127 [==============================] - 133s 62ms/step - loss: 1.1931 - categorical_accuracy: 0.5462 - val_loss: 0.9343 - val_categorical_accuracy: 0.5462 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "2127/2127 [==============================] - 133s 63ms/step - loss: 1.1822 - categorical_accuracy: 0.5492 - val_loss: 0.9739 - val_categorical_accuracy: 0.4801 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "2127/2127 [==============================] - 131s 62ms/step - loss: 1.1557 - categorical_accuracy: 0.5664 - val_loss: 0.9010 - val_categorical_accuracy: 0.5796 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "2127/2127 [==============================] - 130s 61ms/step - loss: 1.1286 - categorical_accuracy: 0.5790 - val_loss: 0.9383 - val_categorical_accuracy: 0.5272 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "2127/2127 [==============================] - 130s 61ms/step - loss: 1.0694 - categorical_accuracy: 0.6198 - val_loss: 0.8302 - val_categorical_accuracy: 0.6114 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "2127/2127 [==============================] - 131s 62ms/step - loss: 1.0286 - categorical_accuracy: 0.6318 - val_loss: 0.8778 - val_categorical_accuracy: 0.5458 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "2127/2127 [==============================] - 136s 64ms/step - loss: 1.0361 - categorical_accuracy: 0.6293 - val_loss: 0.7881 - val_categorical_accuracy: 0.6514 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "2127/2127 [==============================] - 139s 65ms/step - loss: 0.9928 - categorical_accuracy: 0.6551 - val_loss: 0.7414 - val_categorical_accuracy: 0.6624 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "2127/2127 [==============================] - 141s 66ms/step - loss: 0.9967 - categorical_accuracy: 0.6499 - val_loss: 0.7618 - val_categorical_accuracy: 0.6695 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "2127/2127 [==============================] - 135s 64ms/step - loss: 0.9780 - categorical_accuracy: 0.6681 - val_loss: 0.7943 - val_categorical_accuracy: 0.6370 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "2127/2127 [==============================] - 131s 62ms/step - loss: 0.9402 - categorical_accuracy: 0.6809 - val_loss: 0.9304 - val_categorical_accuracy: 0.6434 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "2127/2127 [==============================] - 133s 62ms/step - loss: 0.9381 - categorical_accuracy: 0.6828 - val_loss: 0.7146 - val_categorical_accuracy: 0.6813 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "2127/2127 [==============================] - 133s 62ms/step - loss: 0.9135 - categorical_accuracy: 0.6915 - val_loss: 0.8151 - val_categorical_accuracy: 0.6380 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "2127/2127 [==============================] - 132s 62ms/step - loss: 0.9280 - categorical_accuracy: 0.6869 - val_loss: 0.8182 - val_categorical_accuracy: 0.6478 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "2127/2127 [==============================] - 133s 62ms/step - loss: 0.9573 - categorical_accuracy: 0.6838 - val_loss: 0.8630 - val_categorical_accuracy: 0.7052 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "2127/2127 [==============================] - 132s 62ms/step - loss: 0.9051 - categorical_accuracy: 0.7032 - val_loss: 1.3801 - val_categorical_accuracy: 0.5984 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "2127/2127 [==============================] - 134s 63ms/step - loss: 0.8964 - categorical_accuracy: 0.7049 - val_loss: 0.7696 - val_categorical_accuracy: 0.6547 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "2127/2127 [==============================] - 133s 63ms/step - loss: 0.9021 - categorical_accuracy: 0.7009 - val_loss: 0.7332 - val_categorical_accuracy: 0.6634 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "1296/2127 [=================>............] - ETA: 46s - loss: 0.9078 - categorical_accuracy: 0.6983"
     ]
    }
   ],
   "source": [
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor=\"val_categorical_accuracy\", \n",
    "                  patience=50,\n",
    "                  restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_categorical_accuracy\",\n",
    "                      factor=0.50, patience=50,\n",
    "                      verbose=1,\n",
    "                      min_delta=0.0001),\n",
    "    #ModelCheckpoint(filepath='/content/drive/MyDrive/checkpoints/alex_net_emotions_weighted.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "]\n",
    "build_model(keras_tuner.HyperParameters())\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_categorical_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"./models/\",\n",
    "    project_name=\"emo_detect\"\n",
    ")\n",
    "\n",
    "print(tuner.search_space_summary())\n",
    "tuner.search(train_generator,\n",
    "                    epochs=100,\n",
    "                    validation_data=test_generator,\n",
    "                    callbacks=my_callbacks,\n",
    "                    class_weight=class_weights,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "models = tuner.get_best_models(num_models=5)\n",
    "print(tuner.results_summary())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKTsphyZMRZo",
    "outputId": "9c380df9-bc0b-4fa0-f43f-15d991328e8f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's display everything and save the images!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "um-xf08F56bm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def save_summary(file_path, model):\n",
    "    with open(file_path, 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(model.summary())\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.save(f'./models/model_{i}.h5')\n",
    "    save_summary(f'./details/summaries/model_{i}.txt', model)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_generator)\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(test_generator)\n",
    "    y_actual = test_generator.classes\n",
    "    y_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "    y_pred = np.round(y_pred)\n",
    "\n",
    "    confusion_mtx = confusion_matrix(y_actual, y_pred)\n",
    "    print(confusion_mtx)\n",
    "\n",
    "    # Evaluation\n",
    "    print(classification_report(test_generator.classes, y_pred))\n",
    "\n",
    "    plt.imshow(confusion_mtx, cmap='binary', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, ['Happy', 'Neutral', 'Sad'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['Happy', 'Neutral', 'Sad'])\n",
    "\n",
    "    thresh = confusion_mtx.max() / 2.\n",
    "    for i in range(confusion_mtx.shape[0]):\n",
    "        for j in range(confusion_mtx.shape[1]):\n",
    "            plt.text(j, i, format(confusion_mtx[i, j]), ha=\"center\", va=\"center\", color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Model {i} Confusion Matrix')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig(f'./details/cm_auto_detect/model_{i}_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}